---
title: "自动攒批处理"
layout: post
date: '2024-12-13 17:00:00'
lang: "en"
translated: "/ate-core-devs-tem-atrapalhos"
author_note: "Você pode ler esse artigo em Português"
author_note_link: "https://jtemporal.com/ate-core-devs-tem-atrapalhos"
type: post
tags:
- rocketmq5
- consume
comments: true
---


# 1. 手动批量消息

```Java
List<Message> messages = new ArrayList<>();
messages.add(new Message(TOPIC, TAG, "OrderID001", "Hello world 0".getBytes(StandardCharsets.UTF_8)));
messages.add(new Message(TOPIC, TAG, "OrderID002", "Hello world 1".getBytes(StandardCharsets.UTF_8)));
messages.add(new Message(TOPIC, TAG, "OrderID003", "Hello world 2".getBytes(StandardCharsets.UTF_8)));

SendResult sendResult = producer.send(messages);
```

传统的批量消息发送需要传入一个消息列表，并将消息列表组装成MessageBatch消息进行发送，SendMessageProcessor处理消息时，会将MessageBatch解包成多条消息，依次写入CommitLog，构建索引。

所以后续流程就跟单条消息一样处理了，包括逐条生成ConsumeQueue，消息拉取时根据ConusmeQueue查找消息等。

![image-20240117142055808](https://typora-1259778123.cos.ap-nanjing.myqcloud.com/undefined202401171420852.png)

**缺点：**

1. **批量大小固定，不灵活，效率不高**
2. **只节约了网络层面的开销，而缺少对CPU、磁盘层面的优化**。

org.apache.rocketmq.broker.processor.SendMessageProcessor#sendBatchMessage

```Java
if (this.brokerController.getBrokerConfig().isAsyncSendEnable()) {
    CompletableFuture<PutMessageResult> asyncPutMessageFuture;
    if (isInnerBatch) {
        // 自动攒批的消息，写入单条消息
        asyncPutMessageFuture = this.brokerController.getMessageStore().asyncPutMessage(messageExtBatch);
    } else {
        // 传统批量消息写入，拆分并写入多条消息
        asyncPutMessageFuture = this.brokerController.getMessageStore().asyncPutMessages(messageExtBatch);
    }
```



**拆分批量消息并写入CommitLog**

org.apache.rocketmq.store.CommitLog.DefaultAppendMessageCallback#doAppend(long, java.nio.ByteBuffer, int, org.apache.rocketmq.common.message.MessageExtBatch, org.apache.rocketmq.store.PutMessageContext)

```Java
public AppendMessageResult doAppend(final long fileFromOffset, final ByteBuffer byteBuffer, final int maxBlank,
            final MessageExtBatch messageExtBatch, PutMessageContext putMessageContext) {
            byteBuffer.mark();
            //physical offset
            long wroteOffset = fileFromOffset + byteBuffer.position();
            // Record ConsumeQueue information
            Long queueOffset = messageExtBatch.getQueueOffset();
            long beginQueueOffset = queueOffset;
            int totalMsgLen = 0;
            int msgNum = 0;

            final long beginTimeMills = CommitLog.this.defaultMessageStore.now();
            // 这个ByteBuffer就是消息本身，前面预先编码过，但是缺少一些信息如queueoffset
            ByteBuffer messagesByteBuff = messageExtBatch.getEncodedBuff();

            int sysFlag = messageExtBatch.getSysFlag();
            int bornHostLength = (sysFlag & MessageSysFlag.BORNHOST_V6_FLAG) == 0 ? 4 + 4 : 16 + 4;
            int storeHostLength = (sysFlag & MessageSysFlag.STOREHOSTADDRESS_V6_FLAG) == 0 ? 4 + 4 : 16 + 4;
            Supplier<String> msgIdSupplier = () -> {
                int msgIdLen = storeHostLength + 8;
                int batchCount = putMessageContext.getBatchSize();
                long[] phyPosArray = putMessageContext.getPhyPos();
                ByteBuffer msgIdBuffer = ByteBuffer.allocate(msgIdLen);
                MessageExt.socketAddress2ByteBuffer(messageExtBatch.getStoreHost(), msgIdBuffer);
                msgIdBuffer.clear();//because socketAddress2ByteBuffer flip the buffer

                StringBuilder buffer = new StringBuilder(batchCount * msgIdLen * 2 + batchCount - 1);
                for (int i = 0; i < phyPosArray.length; i++) {
                    msgIdBuffer.putLong(msgIdLen - 8, phyPosArray[i]);
                    String msgId = UtilAll.bytes2string(msgIdBuffer.array());
                    if (i != 0) {
                        buffer.append(',');
                    }
                    buffer.append(msgId);
                }
                return buffer.toString();
            };

            messagesByteBuff.mark();
            int index = 0;
           // 从这个循环可以看出来对批量消息进行了拆分，并依次写入CommitLog
            while (messagesByteBuff.hasRemaining()) {
                // 1 TOTALSIZE
                final int msgPos = messagesByteBuff.position();
                final int msgLen = messagesByteBuff.getInt();

                totalMsgLen += msgLen;
                // Determines whether there is sufficient free space
                if ((totalMsgLen + END_FILE_MIN_BLANK_LENGTH) > maxBlank) {
                    this.msgStoreItemMemory.clear();
                    // 1 TOTALSIZE
                    this.msgStoreItemMemory.putInt(maxBlank);
                    // 2 MAGICCODE
                    this.msgStoreItemMemory.putInt(CommitLog.BLANK_MAGIC_CODE);
                    // 3 The remaining space may be any value
                    //ignore previous read
                    messagesByteBuff.reset();
                    // Here the length of the specially set maxBlank
                    byteBuffer.reset(); //ignore the previous appended messages
                    byteBuffer.put(this.msgStoreItemMemory.array(), 0, 8);
                    return new AppendMessageResult(AppendMessageStatus.END_OF_FILE, wroteOffset, maxBlank, msgIdSupplier, messageExtBatch.getStoreTimestamp(),
                        beginQueueOffset, CommitLog.this.defaultMessageStore.now() - beginTimeMills);
                }
                //move to add queue offset and commitlog offset
                int pos = msgPos + 20;
                messagesByteBuff.putLong(pos, queueOffset);
                pos += 8;
                // 写入缺少的physicalPosition
                messagesByteBuff.putLong(pos, wroteOffset + totalMsgLen - msgLen);
                // 8 SYSFLAG, 9 BORNTIMESTAMP, 10 BORNHOST, 11 STORETIMESTAMP
                pos += 8 + 4 + 8 + bornHostLength;
                // refresh store time stamp in lock
                messagesByteBuff.putLong(pos, messageExtBatch.getStoreTimestamp());
                if (enabledAppendPropCRC) {
                    //append crc32
                    int checkSize = msgLen - crc32ReservedLength;
                    ByteBuffer tmpBuffer = messagesByteBuff.duplicate();
                    tmpBuffer.position(msgPos).limit(msgPos + checkSize);
                    int crc32 = UtilAll.crc32(tmpBuffer);
                    messagesByteBuff.position(msgPos + checkSize);
                    MessageDecoder.createCrc32(messagesByteBuff, crc32);
                }

                putMessageContext.getPhyPos()[index++] = wroteOffset + totalMsgLen - msgLen;
                queueOffset++;
                msgNum++;
                messagesByteBuff.position(msgPos + msgLen);
            }

            messagesByteBuff.position(0);
            messagesByteBuff.limit(totalMsgLen);
            byteBuffer.put(messagesByteBuff);
            messageExtBatch.setEncodedBuff(null);
            AppendMessageResult result = new AppendMessageResult(AppendMessageStatus.PUT_OK, wroteOffset, totalMsgLen, msgIdSupplier,
                messageExtBatch.getStoreTimestamp(), beginQueueOffset, CommitLog.this.defaultMessageStore.now() - beginTimeMills);
            result.setMsgNum(msgNum);

            return result;
        }

    }
```

# 2. 自动攒批处理

```Java
producer.setAutoBatch(true);
producer.batchMaxBytes(32*1024);
producer.batchMaxDelayMs(10);
producer.totalBatchMaxBytes(32*1024*1024);
result = producer.send(msg);
```

batchMaxBytes：每个MessageAccumulation最大累积大小，超过该阈值将会打包发送

batchMaxDelayMs：每条Message在暂存区最长等待时间，超过该阈值将会打包发送

totalBatchMaxBytes：所有MessageAccumulation累积大小之和的阈值，超过该阈值后续请求将直接发送

当客户端开启了自动攒批后，客户端会对同一个Topic+tag的消息进行攒批发送，积攒5ms或者积攒了32KB的消息后机进行发送

```Java
private boolean readyToSend() {
    if (this.messagesSize.get() > holdSize
        || System.currentTimeMillis() >= this.createTime + holdMs) {
        return true;
    }
    return false;
}
```

注意，每个ClientID会初始化一个ProduceAccumulator，每个Topic + tag会初始化一个MessageAccumulation（主要是由于写入CommitLog的时候需要视作同一条消息，即topic和tag相同）

```Java
    SendResult send(Message msg, MessageQueue mq,
        DefaultMQProducer defaultMQProducer) throws InterruptedException, MQBrokerException, RemotingException, MQClientException {
        // 覆写了equals和hash方法，可以用作map的key
        AggregateKey partitionKey = new AggregateKey(msg, mq);
        while (true) {
             // 一个ClientID对应到一个攒批器，Topic+tag区分MessageAccumulation
            MessageAccumulation batch = getOrCreateSyncSendBatch(partitionKey, defaultMQProducer);
            // 尝试将该条消息攒批，如果成功就返回这条消息的索引（如果不成功该线程会wait状态，直到其他线程发送成功并通知所有线程
            // 或者guard线程定时唤醒
            int index = batch.add(msg);
            if (index == -1) {
                syncSendBatchs.remove(partitionKey, batch);
            } else {
                // 根据索引获取发送结果
                return batch.sendResults[index];
            }
        }
    }
        public int add(
            Message msg) throws InterruptedException, MQBrokerException, RemotingException, MQClientException {
            int ret = -1;
            synchronized (this.closed) {
                if (this.closed.get()) {
                    return ret;
                }
                ret = this.count++;
                this.messages.add(msg);
                messagesSize.addAndGet(msg.getBody().length);
                String msgKeys = msg.getKeys();
                if (msgKeys != null) {
                    this.keys.addAll(Arrays.asList(msgKeys.split(MessageConst.KEY_SEPARATOR)));
                }
            }
            synchronized (this) {
                while (!this.closed.get()) {
                    if (readyToSend()) {
                        this.send();
                        break;
                    } else {
                        // 发送线程在MessageAccumulation对象上wait
                        this.wait();
                    }
                }
                // 当有一个线程发送成功后，唤醒等待在MessageAccumulation对象上的线程
                return ret;
            }
        }
```

其线程工作流程概况如下：

多个线程发送时，竞争一个`MessageAccumulation`对象的锁，如果未达成发送条件，进入waitting状态；当某个线程达成发送条件后，将消息攒批发送，获得发送结果，同时唤醒所有等待在该对象上的线程，根据index拆分发送结果，发送流程结束。为了防止线程全部进入waitting状态，设置后台守护线程`GuardForSyncSendService` ，其定时任务会notify等待在该对象上的线程。

![image-20240117141952649](https://typora-1259778123.cos.ap-nanjing.myqcloud.com/undefined202401171419705.png)

# 3. 批量写入

只要开启了autobatch属性后，就可以使用自动攒批发送功能，但是如果想要对MessageBatch不作拆分直接写入CommitLog，该特性对Topic属性有要求，只有queue.type=BatchCQ的主题才支持BatchConsumeQueue。

```sh
sh /apps/svr/rockemq5/bin/mqadmin updatetopic -n localhost:19888 -t test3 -c DefaultCluster -a +message.type=DELAY,+queue.type=BatchCQ
create topic to 10.253.151.69:11911 success.
TopicConfig [topicName=test3, readQueueNums=8, writeQueueNums=8, perm=RW-, topicFilterType=SINGLE_TAG, topicSysFlag=0, order=false, attributes={+queue.type=BatchCQ, +message.type=DELAY}]
```



```Bash

if (QueueTypeUtils.isBatchCq(Optional.of(topicConfig)) && MessageClientIDSetter.getUniqID(messageExtBatch) != null) {
    // newly introduced inner-batch message
    messageExtBatch.setSysFlag(messageExtBatch.getSysFlag() | MessageSysFlag.NEED_UNWRAP_FLAG);
    messageExtBatch.setSysFlag(messageExtBatch.getSysFlag() | MessageSysFlag.INNER_BATCH_FLAG);
    // 当topic包含batch特性时，设置batch属性表明直接写入CommitLog
    messageExtBatch.setInnerBatch(true);

    int innerNum = MessageDecoder.countInnerMsgNum(ByteBuffer.wrap(messageExtBatch.getBody()));

    MessageAccessor.putProperty(messageExtBatch, MessageConst.PROPERTY_INNER_NUM, String.valueOf(innerNum));
    messageExtBatch.setPropertiesString(MessageDecoder.messageProperties2String(messageExtBatch.getProperties()));

    // tell the producer that it's an inner-batch message response.
    responseHeader.setBatchUniqId(MessageClientIDSetter.getUniqID(messageExtBatch));

    isInnerBatch = true;
}
```

批量消息有两种，一种是自己打包的批量消息，一种是自动攒批的批量消息，当MessageBatch消息发送后，**如果是queue.type=BatchCQ的Topic，并且是自动攒批的，则会直接当作单条消息写入CommitLog，否则按照传统批量消息拆分写入**

BatchConsumeQueue介绍

![image-20240103152223110](https://typora-1259778123.cos.ap-nanjing.myqcloud.com/undefined202401031522150.png)

ConsumeQueue的作用就是在处理拉取请求时，由于CommitLog是顺序写的，需要CQ找到消息的位置。因此CQ中包含了消息在CommigLog的offset、消息大小、tag的哈希。

**BatchConsumeQueue为什么要这么设计呢？**

在自动攒批时，CommitLog中的一条消息数据，实际上是多条消息的集合，所以需要知道这条消息是几条消息攒批的，即batchSize属性（返回给客户端拉消息数量）

批量消息的msgBaseOffset，就是消息写入CommitLog时保存的QueueOffset，即在某个queue中的位点。





CommitLog结构如下：

![img](https://typora-1259778123.cos.ap-nanjing.myqcloud.com/undefined202401171421287.png)

可以看到：BatchConsumeQueue将一些数据从CommitLog中带出来了，增大了CQ的复杂度。



# 4. PullMessage

处理PULL请求时，根据BCQ查到消息在commitlog中的位置，然后将查找到的字节数据交给客户端处理。

注意这时候**查出来的消息也是一个MessageBatchExt，CommitLog的格式是固定的，服务端不解析批量的消息**，真正还原消息是在客户端处理的！



**org.apache.rocketmq.store.GetMessageResult#addMessage(org.apache.rocketmq.store.SelectMappedBufferResult, long)**

```Java
public void addMessage(final SelectMappedBufferResult mapedBuffer, final long queueOffset) {
    this.messageMapedList.add(mapedBuffer);
    this.messageBufferList.add(mapedBuffer.getByteBuffer());
    this.bufferTotalSize += mapedBuffer.getSize();
    this.msgCount4Commercial += (int) Math.ceil(
        mapedBuffer.getSize() /  (double)commercialSizePerMsg);
    this.messageCount++;
    // 客户端如何处理GetResult？
    this.messageQueueOffset.add(queueOffset);
}


public void addMessage(final SelectMappedBufferResult mapedBuffer, final long queueOffset, final int batchNum) {
    addMessage(mapedBuffer, queueOffset);
    // 拉取到批量消息后更新消息数量
    messageCount += batchNum - 1;
}
```

客户端处理逻辑：

```Java
    public PullResult processPullResult(final MessageQueue mq, final PullResult pullResult,
        final SubscriptionData subscriptionData) {
        PullResultExt pullResultExt = (PullResultExt) pullResult;

        this.updatePullFromWhichNode(mq, pullResultExt.getSuggestWhichBrokerId());
        if (PullStatus.FOUND == pullResult.getPullStatus()) {
            ByteBuffer byteBuffer = ByteBuffer.wrap(pullResultExt.getMessageBinary());
            // response中保存了字节数组，按照commitlog组成解析成消息列表
            List<MessageExt> msgList = MessageDecoder.decodesBatch(
                byteBuffer,
                this.mQClientFactory.getClientConfig().isDecodeReadBody(),
                this.mQClientFactory.getClientConfig().isDecodeDecompressBody(),
                true
            );

            boolean needDecodeInnerMessage = false;
            // 由于PULL请求是对MessageQueue级别的，所以每条消息都是打包过的需要解包
            for (MessageExt messageExt: msgList) {
                if (MessageSysFlag.check(messageExt.getSysFlag(), MessageSysFlag.INNER_BATCH_FLAG)
                    && MessageSysFlag.check(messageExt.getSysFlag(), MessageSysFlag.NEED_UNWRAP_FLAG)) {
                    needDecodeInnerMessage = true;
                    break;
                }
            }
            if (needDecodeInnerMessage) {
                List<MessageExt> innerMsgList = new ArrayList<>();
                try {
                    for (MessageExt messageExt: msgList) {
                        if (MessageSysFlag.check(messageExt.getSysFlag(), MessageSysFlag.INNER_BATCH_FLAG)
                            && MessageSysFlag.check(messageExt.getSysFlag(), MessageSysFlag.NEED_UNWRAP_FLAG)) {
                            // 对自动攒批的消息进行解码
                            MessageDecoder.decodeMessage(messageExt, innerMsgList);
                        } else {
                            innerMsgList.add(messageExt);
                        }
                    }
                    msgList = innerMsgList;
                } catch (Throwable t) {
                    log.error("Try to decode the inner batch failed for {}", pullResult.toString(), t);
                }
            }
```



# 5. 总结

1. 在多线程发送时，使用自动打包可以提高吞吐量，但是单线程发送时，反而会降低效率。
2. 批量存储对Topic类型有要求，只有包含了queue.type=batchCQ才能批量存储。
3. 引入了BatchConsumeQueue检索CommitLog中存储的压缩消息，并且扩展为稀疏索引（SparseConsumeQueue），可以满足对消息Compaction特性。





# Q&A

## Q1

>  批量消息攒批发送后，对于commitlog来说是一个消息是吗？那构建consumeQueue的时候也是只有一个cqunit？

对于queue.type=BatchCQ的Topic，批量消息直接存入CommitLog，同时会构建一条BatchConsumeQueue。



发送批量消息

![image-20240118151013193](https://typora-1259778123.cos.ap-nanjing.myqcloud.com/undefined202401181510251.png)



## Q2

>  如果问题1成立，那查询消息只能查询整体的这批消息，单个消息如何获取？

查询消息根据uniquekey或者offsetmsgid都能定位到commitlog中的消息，这里的消息就是MessageBatch，**不存在单条消息的概念**。如果是非自动攒批消息，也就是分开写入CommitLog，那么根据msgID能查到单条消息。

可以看到自动攒批批量消息有同一个MsgId跟offsetId，表明消息在commitlog中存储在一个单元中

![image-20240118103806033](https://typora-1259778123.cos.ap-nanjing.myqcloud.com/undefined202401181038085.png)

通过offsetMsgId查询到,INNER_NUMS=5表明这批消息是由5条消息攒批发送的

![image-20240118102652351](https://typora-1259778123.cos.ap-nanjing.myqcloud.com/undefined202401181026400.png)

通过UniqueKey查询到

![image-20240118102702499](https://typora-1259778123.cos.ap-nanjing.myqcloud.com/undefined202401181027539.png)



## Q3

> 消费者拉取批量消息这个特殊消息后，在客户端进行解析成多条消息进行消费，那如果这批消息中的某条消息消费失败，如何处理？

客户端拉取这条MessageBatch后，客户端解包成多条消息，并且每条消息的MsgID也不一样，当消费失败后，重发这条消息回Broker重试。

消费批量消息

![image-20240118151041288](https://typora-1259778123.cos.ap-nanjing.myqcloud.com/undefined202401181510354.png)



# ref

[Both CQ and BCQ need to be supported in DefaultMessageStore](https://github.com/apache/rocketmq/issues/3708)[support batch consume-queue.]

[Improve Batch Message Processing Throughput ](https://github.com/apache/rocketmq/pull/3671)[Both CQ and BCQ need to be supported in DefaultMessageStore]

